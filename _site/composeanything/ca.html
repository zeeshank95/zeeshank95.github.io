<script src="http://www.google.com/jsapi" type="text/javascript"></script>

<style>
    .button {
        background-color: #4CAF50;
        /* Green */
        border: none;
        color: white;
        padding: 10px 20px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 5px 10px;
        cursor: pointer;
    }

    .button1 {
        border-radius: 5px;
        /* font-size: 18px; */
        background-color: white;
        color: black;
        border: 2px solid #2d6987
        /* Green */
    }

    .button2 {
        border-radius: 4px;
    }

    .button3 {
        border-radius: 8px;
    }

    .button4 {
        border-radius: 12px;
    }

    .button5 {
        border-radius: 50%;
    }


    .td-center {
        align: center;
        text-align: center;
        padding: 10px
    }

    .text_div {
        text-align: justify;
        text-justify: inter-word;
    }

    #blink {
        color: red;
        transition: 0.4s;
    }
    .crop {
        width: 650px;
        
        overflow: hidden;
    }
    .crop1 {
        width: 650px;
        margin: -75px 0 -75px 0px;
    }
</style>


<html lang="en">
    <head>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-W1LDHVTXGV"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-W1LDHVTXGV');
        </script>
        <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel='icon' href='favicon.ico' type='image/x-icon'/>
        <meta name="description" content="ComposeAnything: Composite Object Priors for Text-to-Image Generation">
        <meta name="author" content="WILLOW team">
        <title>ComposeAnything: Composite Object Priors for Text-to-Image Generation</title>
        <link href="css/bootstrap.min.css" rel="stylesheet">
    </head>

    <body>
    <div class="container">

        <div style="height:20px;"></div>
        <div class="header">
            <h3>
                <center> <b>ComposeAnything: Composite Object Priors for Text-to-Image Generation</b> </center>
            </h3>
        </div>
        <div style="height:10px;"></div>

        <table align=center max-width=900px>
            <tr>
                <td align=center width=200px>
                    <center>
                        <span style="font-size:18px"><a href="https://zeeshank95.github.io" target="_blank">Zeeshan Khan</a></span>
                    </center>
                </td>
                <td align=center width=200px>
                    <center>
                        <span style="font-size:18px"><a href="https://cshizhe.github.io/" target="_blank">Shizhe Chen</a></span>
                    </center>
                </td>
                <td align=center width=200px>
                    <center>
                        <span style="font-size:18px"><a href="https://cordeliaschmid.github.io/" target="_blank">Cordelia Schmid</a></span>
                    </center>
                </td>
            </tr>
        </table>
        <div style="height:5px;"></div>

        <table align=center max-width=650px>
            <tr>
                <td align=center width=500px>
                    <center>
                        <span style="font-size:16px"><sup>1</sup>Inria, École normale supérieure, CNRS, PSL Research University</span>
                    </center>
                </td>
            </tr>
        </table>
        <div style="height:10px;"></div>

        <table align=center width=700px>
			<tr>
				<td align=center width=700px>
					<center>
						<span style="font-size:16px">Under Review</span>
					</center>
				</td>
			</tr>
		</table>
		<div style="height:10px;"></div>

        <div class="links" style="font-weight:bold; text-align:center">
            <a class="btn btn-info" href="https://zeeshank95.github.io/composeanything/ca.html" target="_blank">Paper</a>
            &nbsp;&nbsp;&nbsp;
            <a class="btn btn-info" href="hort.html#bib">BibTex</a>
            &nbsp;&nbsp;&nbsp;
            <a class="btn btn-info" href="https://zeeshank95.github.io/composeanything/ca.html" target="_blank">Code</a>
            &nbsp;&nbsp;&nbsp;
        </div>

        <hr>

        <div class="row" id="method" style="max-width:1000px; margin:0 auto; text-align:justify">
            <center>
                <img src="resources/CA_assets/CA_teaser.pdf" width="100%">
            </center>
        </div>

        <div class="row" id="abstract" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Abstract</h3>
            <p style="text-align: justify;">
                Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition. Serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text. Moreover our prior guided diffusion, enable flexible and interpretable generation over attributes like appearance, position, and counts by modulating the strength of the prior.
	        </p>
        </div><!-- Import the component -->

        <hr>

        <div class="row" id="method" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Method</h3><br>
            <center>
                <img src="resources/CA_assets/method.png" width="100%">
            </center>
            <br>
            <p style="text-align: justify;">
                <b>Our approach.</b> ComposeAnything framework consists of four key components for compositional text-to-image generation: (a) LLM Planning: We employ LLMs to transform the input prompt into a structured 2.5D semantic layout, including object captions, bounding boxes and relative depths; (b) Composite Object Prior: Based on the layout, we generate a coarse composite image that serves as a strong apperance, semantic and, spatial prior, for guiding image generation; and (c) Prior Guided Diffusion: During generation we, replace random noise initialise with a noisy object prior, and iteratively reinforce the object prior. We also apply spatially-controlled self-attention to preserve semantics and structure in early denoising steps. (d) Standard Diffusion: After a few steps when the global structure if formed, we remove all the control and use standard denoising.
            </p>
            <br>
        </div>

        <hr>

        <div class="row" id="qual" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>SOTA Comparison</h3><br>
            <center>
                <img src="resources/CA_assets/table.png" width="100%">
            </center>
            <br>
            <p style="text-align: justify;">
                <b>State-of-the-art Comparison.</b> We compoare our method with both (a) training based box-conditioned models (LayoutGPT and CreatiLayout), and (b) Inference only T2I Diffusion models, including standard base models like SDXL, SD3-M, FLUX, layout based models (RealCompo and RPG) and Noise search method (Inference time scaling). Our approch outperform all the existing methods, both training-based and training-free. 
            </p>
            <br>
        </div>        

        <hr>
        
        <div class="row" id="qual" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Qualitative Results</h3><br>
            <center>
                <img src="resources/CA_assets/results.png" width="100%">
            </center>
            <br>
            <p style="text-align: justify;">
                <b>Qualitative performance.</b> Qualitative results of the proposed model on T2I-Compbench- We show the input prompt, the LLM generated composite object prior, and the corresponding generated image.
            </p>
            <br>
        </div>

        <hr>

        <div class="row" id="bib" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>BibTeX</h3>
               <pre><tt>@InProceedings{khan2025composeanything,
author       = {Khan, Zeeshan and Chen, Shizhe and Schmid, Cordelia},
title        = {{ComposeAnythin}: Composite Object Priors for Text-to-Image Generation},
booktitle    = {arXiv},
year         = {2025},
}</tt></pre>
        </div>

        <hr>

        <div class="row" id="acknowledgements" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Acknowledgements</h3>
            <p>
                This work was granted access to the HPC resources of IDRIS under the allocation AD011014688R1 made by GENCI. 
It was funded in part by the French government under management of Agence Nationale de la Recherche as part of the “France 2030" program, reference ANR-23-IACL-0008 (PR[AI]RIE-PSAI projet), and Paris Île-de-France Région in the frame of the DIM AI4IDF. Cordelia Schmid would like to acknowledge the support by the Körber European Science Prize.
            </p>
        </div>

        <hr>

        <div class="row" id="copyright" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Copyright</h3>
            <p>
                The documents contained in these directories are included by the contributing authors as a means to ensure timely dissemination of scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright.
            </p>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-3 col-sm-9">
                <center>
                   <img src="resources/inria_logo.png" width="150">
                </center>
            </div>
            <div class="col-md-3 col-sm-9">
                <center>
                   <img src="resources/ens_logo.png" width="150">
                </center>
            </div>
        </div>

   </body>
</html>
