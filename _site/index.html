<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Zeeshan Khan - Home</title>
  <meta name="description" content="Zeeshan Khan">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>

    <a class="navbar-brand" href="http://localhost:4000/">Zeeshan Khan</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li><a href="http://localhost:4000/publications">Publications</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <div class="container-fluid">

  <div class="row">

    <div class="col-sm-8">

      <p>Hi! I am a second year PhD student in the <a href="http://www.di.ens.fr/willow">Willow</a> team at <a href="https://www.inria.fr/en">Inria</a> and  <a href="https://www.ens.psl.eu/en">École Normale Supérieure</a> in Paris, advised by <a href="https://cordeliaschmid.github.io">Cordelia Schmid</a> and <a href="https://cshizhe.github.io">Shizhe Chen</a>. I am working on Diffusion models for vision-language generation.</p>

      <p>I did my Masters by Research in Computer Science from <a href="http://cvit.iiit.ac.in">CVIT IIIT Hyderabad</a> advised by <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a> and <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a>. My thesis was on Situation Recognition for Holistic Video Understanding, exploring structured representations to enable deeper semantic understanding of dynamic scenes. Prior to this I was a Research Assistant in the Computer Vision lab at <a href="https://iitgn.ac.in">IIT Gandhinagar</a>, where I worked with <a href="https://shanmuga.people.iitgn.ac.in"> Shanmuganathan Raman</a>. My work centered around Computational Photography, specifically in high dynamic range (HDR) image and video reconstruction.</p>

      <p>I am broadly interested in unified multimodal models particularly at the intersection of diffusion models and autoregressive large language models for joint understanding and generation across text, images, and videos. Currently I am exploring compositional representations for high fidelity and interpretable text-to-image/video diffusion models.</p>

      <p align="center">
  <a href="./docs/Zeeshan_cv_2022.pdf">CV</a> /
  <a href="https://scholar.google.com/citations?user=uvhBVYoAAAAJ&amp;hl=en">Google Scholar</a> /
  <a href="https://github.com/zeeshank95">Github</a> /
  <a href="https://www.linkedin.com/in/khan-zeeshan-606-">LinkedIn</a> /
</p>

      <h3 id="news">News</h3>
      <hr />

      <p>May, 2025 :
<em>Released our new work <a href="https://zeeshank95.github.io/composeanything/ca.html">ComposeAnything</a>, for compositional text to image gneration.</em></p>

      <p>February, 2025 :
<em>One Paper accepted to <a href="https://cvpr.thecvf.com">CVPR 2025</a>!. We create a challenging benchmark for compositional video understanding.</em></p>

      <p>February, 2024 :
<em>One Paper accepted to <a href="https://cvpr.thecvf.com">CVPR 2024</a>!. We design a new framework for Identity aware captioning of movie videos, we also propose a new captioning metric called iSPICE, that is sensitive to wrong identiities in captions.</em></p>

      <p>September, 2023 :
<em>Started PhD in the <a href="https://www.di.ens.fr/willow/">Willow</a> team of <a href="https://www.inria.fr/fr">Inria</a> Paris.</em></p>

      <p>September, 2022 :
<em>One paper accepted to <a href="https://neurips.cc">NeurIPS 2022</a>! We formulate a new structured framework for dense video understanding and propose a Transformer based model, VideoWhisperer that operates on a group of clips and jointly predicts all the salient actions, Semantic roles via captioning and, spatio temporal grounding in a weakly supervised setting.</em></p>

      <h4 id="see-all-news"><a href="http://localhost:4000/allnews.html">See all news</a></h4>

    </div>

    <div class="col-sm-4" style="display:table-cell; vertical-align:left; text-align:left">

      <div class="text-left">
        <ul style="overflow: hidden">
  <img src="http://localhost:4000/images/profile_pic.jpeg" class="img-responsive" width="100%" />
  </ul>

        <p><!-- <br clear="all" /> -->
  <a href="mailto:zeeshan.khan@inria.fr">zeeshan.khan@inria.fr</a> <br />
  Office: C-412<br />
  Address: 2 Rue Simone IFF, 75012 Paris France<br /></p>

      </div>

    </div>
  </div>

  <div class="col-sm-12">

    <h3 id="publications">Publications</h3>
    <hr />

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>ComposeAnything: Composite Object Priors for Text-to-Image Generation</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/CA_teaser.pdf" class="img-responsive" width="250px" style="float: left" /></p>

        <p>ComposeAnything framework enables interpretable text-to-image generation for complex compositions involving surreal 2D, 3D spatial relationships and high object counts.</p>

        <p><em><b>Zeeshan Khan</b>, <a href="https://cshizhe.github.io">Shizhe Chen</a>, <a href="https://cordeliaschmid.github.io">Cordelia Schmid</a></em></p>

        <p><i> under review </i></p>

        <p><a href="https://zeeshank95.github.io/composeanything/ca.html">Paper</a>
 /
 <a href="https://zeeshank95.github.io/composeanything/ca.html">Project Page</a>
 /
 <a href="https://zeeshank95.github.io/composeanything/ca.html">Code (Github)</a></p>

      </div>
    </div>

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>VELOCITI: Can Video-Language Models Bind Semantic Concepts Through Time?</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/velociti.png" class="img-responsive" width="250px" style="float: left" /></p>

        <p>We create a new benchmark to evaluate video VLMs both contrastive and LLM based. The tasks are designed to evaluate fine-grained compositional understanding abilities of VLMs. All of the open-source VLMs perform close to random. Gemini outperforms all but with a significant gap to human performance.</p>

        <p><em>Darshana Saravanan,  Darshan Singh, Varun Gupta, <b>Zeeshan Khan</b>, Vineet Gandhi, <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a>,</em></p>

        <p><i> In the Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2025</i></p>

        <p><a href="https://arxiv.org/abs/2406.10889">Paper</a>
 /
 <a href="https://katha-ai.github.io/projects/velociti/">Project Page</a>
 /
 <a href="https://github.com/katha-ai/VELOCITI">Code (Github)</a></p>

      </div>
    </div>

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>MICap: A Unified Model for Identity-aware Movie Descriptions</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/micap.png" class="img-responsive" width="250px" style="float: left" /></p>

        <p>We design a single stage framework for Identity aware captioning of movie videos, we also propose a new captioning metric called <i>iSPICE</i>, that is sensitive to wrong identiities in captions.</p>

        <p><em>Haran Raajesh, Naveen Reddy Desanur, <b>Zeeshan Khan</b>, <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a>,</em></p>

        <p><i> In the Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2024</i></p>

        <p><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Raajesh_MICap_A_Unified_Model_for_Identity-Aware_Movie_Descriptions_CVPR_2024_paper.pdf">Paper</a>
 /
 <a href="https://katha-ai.github.io/projects/micap/">Project Page</a>
 /
 <a href="https://github.com/katha-ai/MovieIdentityCaptioner-CVPR2024">Code (Github)</a></p>

      </div>
    </div>

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>Grounded Video Situtation Recognition</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/Neurips.jpg" class="img-responsive" width="250px" style="float: left" /></p>

        <p>We formulate a new structured framework for dense video understanding and propose a Transformer based model, VideoWhisperer that operates on a group of clips and jointly predicts all the salient actions, Semantic roles via captioning and, spatio temporal grounding in a weakly supervised setting</p>

        <p><em><b>Zeeshan Khan</b>, <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a>, <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a></em></p>

        <p><i>In Neural Information Processing Systems (<b>NeurIPS</b>), 2022</i></p>

        <p><a href="https://arxiv.org/abs/2210.10828">Paper</a>
 /
 <a href="https://zeeshank95.github.io/grvidsitu">Project Page</a>
 /
 <a href="https://zeeshank95.github.io/grvidsitu">Code (Github)</a></p>

      </div>
    </div>

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>More Parameters No Thanks!</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/ACL.png" class="img-responsive" width="250px" style="float: left" /></p>

        <p>We propose to recursively prune and retrain a Transformer to find language dependent submodules that involves 2 type of paramteres, 1)Shared multlingual and 2)Unique Language dependent parameters, to overcome negative interference in Multilingual Neural Machine translation.</p>

        <p><em><b>Zeeshan Khan</b>, Kartheek Akella, <a href="https://vinaypn.github.io"> Vinay Namboodiri</a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a></em></p>

        <p><i>In Association For Computational Linguistics (<b>ACL</b>) (Findings), 2021</i></p>

        <p><a href="https://aclanthology.org/2021.findings-acl.9/">Paper</a>
 /
 <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/more-parameters-no-thanks">Project Page</a>
 /
 <a href="https://github.com/zeeshank95/PF-Adaptation">Code (Github)</a></p>

      </div>
    </div>

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>DeepHS-HDRVideo : Deep High Speed High Dynamic Range Video Reconstruction</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/HDR_video.png" class="img-responsive" width="250px" style="float: left" /></p>

        <p>This is the first attempt towards generating high speed high dynamic range videos from low speed low dynamic range videos. We use video frame interpolation to recursivrly generate the high and low exposure images missing in the input alternative exposure frames. The High and Low exposure frames are merged at each timestep to generate an HDR video.</p>

        <p><em><b>Zeeshan Khan</b>, Parth Shettiwar, Mukul Khanna, <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a></em></p>

        <p><i>In International Conference on Pattern Recognition(<b>ICPR</b>), 2022 <span style="color:red;">(ORAL)</span></i></p>

        <p><a href="https://arxiv.org/abs/2210.04429">Paper</a>
 /
 <a href="https://youtu.be/TWWv7fh0Slk">Video</a></p>

      </div>
    </div>

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>Appearance Consistent Human Pose Transfer via Dynamic Feature Selection</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/Pose_transfer.png" class="img-responsive" width="250px" style="float: left" /></p>

        <p>We present a robut deep architecture for Appearance Consistent person image generation in novel poses. We incorporate a 3 stream network, for image, pose, and appearance. Additionaly we use Gated convolutions and, Non-local attention blocks for generating realistic images.</p>

        <p><em>Ashish Tiwari, <b>Zeeshan Khan</b>, <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a></em></p>

        <p><i>In International Conference on Pattern Recognition (<b>ICPR</b>), 2022</i></p>

        <p><a href="https://zeeshank95.github.io/">Paper</a></p>

      </div>
    </div>

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>Exploring Pair-Wise NMT for Indian Languages</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/ICON.png" class="img-responsive" width="250px" style="float: left" /></p>

        <p>We address the task of improving pair-wise machine translation for low resource Indian languages using a filtered back-translation process and subsequent fine-tuning on the limited pair-wise language corpora</p>

        <p><em>Kartheek Akella, Sai Himal Allu, Sridhar Suresh Ragupathi, Aman Singhal,<b>Zeeshan Khan</b>, <a href="https://vinaypn.github.io"> Vinay Namboodiri</a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a></em></p>

        <p><i>In International Conference on Natural Language Processing(<b>ICON</b>) 2020</i></p>

        <p><a href="https://aclanthology.org/2020.icon-main.59/">Paper</a></p>

      </div>
    </div>

    <div class="col-sm-11 clearfix">
      <div class="well">
        <pubtit>FHDR: HDR Image Reconstruction from a Single LDR Image using Feedback Network</pubtit>

        <p><img src="http://localhost:4000/images/pubpic/HDR_img.png" class="img-responsive" width="250px" style="float: left" /></p>

        <p>Proposed a recurrent Feedback CNN for HDR image reconstruction from a single exposure LDR image, achieving SOTA results on all the HDR benchmarks. Designed a novel Dense Feedback Block using hidden states of RNN, to transfer the high-level information to the low-level features. LDR to HDR representations are learned in multiple iterations via feedback loops.</p>

        <p><em><b>Zeeshan Khan</b>, Mukul khanna, and <a href="https://people.iitgn.ac.in/~shanmuga/">Prof. Shanmuganathan Raman</a></em></p>

        <p><i>In Global Conference on Signal and Information Processing (<b>GlobalSIP</b>) 2019 <span style="color:red;">(ORAL)</span></i></p>

        <p><a href="https://arxiv.org/pdf/1912.11463.pdf">Paper</a>
 /
 <a href="https://github.com/mukulkhanna/FHDR">Code (Github)</a></p>

      </div>
    </div>

    <p><br clear="all" /></p>

    <h4 id="see-all-publications"><a href="http://localhost:4000/publications">See all publications</a></h4>

  </div>

</div>

</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-5">

		  <p>&copy 2022 Zeeshan Khan. Site made with <a href="https://jekyllrb.com">Jekyll</a>.</p>
		   <p>  </p><p>


		</div>
		<div class="col-sm-5">
		</div>
    <div class="col-sm-5">
		</div>
		<div class="col-sm-5">
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
