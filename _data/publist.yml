- title: 'ComposeAnything: Composite Object Priors for Text-to-Image Generation'
  image: CA_teaser.png
  description: ComposeAnything framework enables interpretable text-to-image generation for complex compositions involving surreal 2D, 3D spatial relationships and high object counts.
  authors: <b>Zeeshan Khan</b>, <a href="https://cshizhe.github.io">Shizhe Chen</a>, <a href="https://cordeliaschmid.github.io">Cordelia Schmid</a> 
  venue: <i> under review </i>
  number_link: 3
  link1:
    url: https://arxiv.org/abs/2505.24086
    display: Paper
  link2:
    url: https://zeeshank95.github.io/composeanything/ca.html
    display: Project Page
  link3:
    url: https://github.com/zeeshank95/ComposeAnything
    display: Code (Github)
    

- title: 'VELOCITI: Can Video-Language Models Bind Semantic Concepts Through Time?'
  image: velociti.png
  description: We create a new benchmark to evaluate video VLMs both contrastive and LLM based. The tasks are designed to evaluate fine-grained compositional understanding abilities of VLMs. All of the open-source VLMs perform close to random. Gemini outperforms all but with a significant gap to human performance.
  authors: Darshana Saravanan,  Darshan Singh, Varun Gupta, <b>Zeeshan Khan</b>, Vineet Gandhi, <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a>, 
  venue: <i> In the Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2025</i>
  number_link: 3
  link1:
    url: https://arxiv.org/abs/2406.10889
    display: Paper
  link2:
    url: https://katha-ai.github.io/projects/velociti/
    display: Project Page
  link3:
    url: https://github.com/katha-ai/VELOCITI
    display: Code (Github)


- title: 'MICap: A Unified Model for Identity-aware Movie Descriptions'
  image: micap.png
  description: We design a single stage framework for Identity aware captioning of movie videos, we also propose a new captioning metric called <i>iSPICE</i>, that is sensitive to wrong identiities in captions.
  authors: Haran Raajesh, Naveen Reddy Desanur, <b>Zeeshan Khan</b>, <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a>, 
  venue: <i> In the Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2024</i>
  number_link: 3
  link1:
    url: https://openaccess.thecvf.com/content/CVPR2024/papers/Raajesh_MICap_A_Unified_Model_for_Identity-Aware_Movie_Descriptions_CVPR_2024_paper.pdf
    display: Paper
  link2:
    url: https://katha-ai.github.io/projects/micap/
    display: Project Page
  link3:
    url: https://github.com/katha-ai/MovieIdentityCaptioner-CVPR2024
    display: Code (Github)


- title: 'Grounded Video Situtation Recognition'
  image: Neurips.jpg
  description: We formulate a new structured framework for dense video understanding and propose a Transformer based model, VideoWhisperer that operates on a group of clips and jointly predicts all the salient actions, Semantic roles via captioning and, spatio temporal grounding in a weakly supervised setting
  authors: <b>Zeeshan Khan</b>, <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a>, <a href="https://makarandtapaswi.github.io">Makarand Tapaswi</a>
  venue: <i>In Neural Information Processing Systems (<b>NeurIPS</b>), 2022</i>
  number_link: 3
  link1:
    url: https://arxiv.org/abs/2210.10828
    display: Paper
  link2:
    url: https://zeeshank95.github.io/grvidsitu
    display: Project Page
  link3:
    url: https://zeeshank95.github.io/grvidsitu
    display: Code (Github)

- title: 'More Parameters No Thanks!'
  image: ACL.png
  description: We propose to recursively prune and retrain a Transformer to find language dependent submodules that involves 2 type of paramteres, 1)Shared multlingual and 2)Unique Language dependent parameters, to overcome negative interference in Multilingual Neural Machine translation. 
  authors: <b>Zeeshan Khan</b>, Kartheek Akella, <a href="https://vinaypn.github.io"> Vinay Namboodiri</a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a>
  venue: <i>In Association For Computational Linguistics (<b>ACL</b>) (Findings), 2021</i>
  number_link: 3
  link1:
    url: https://aclanthology.org/2021.findings-acl.9/
    display: Paper
  link2:
    url: http://cvit.iiit.ac.in/research/projects/cvit-projects/more-parameters-no-thanks
    display: Project Page
  link3:
    url: https://github.com/zeeshank95/PF-Adaptation
    display: Code (Github)


- title: "DeepHS-HDRVideo : Deep High Speed High Dynamic Range Video Reconstruction"
  image: HDR_video.png
  description: This is the first attempt towards generating high speed high dynamic range videos from low speed low dynamic range videos. We use video frame interpolation to recursivrly generate the high and low exposure images missing in the input alternative exposure frames. The High and Low exposure frames are merged at each timestep to generate an HDR video.
  authors: <b>Zeeshan Khan</b>, Parth Shettiwar, Mukul Khanna, <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a>
  venue: <i>In International Conference on Pattern Recognition(<b>ICPR</b>), 2022 <span style="color:red;">(ORAL)</span></i>
  number_link: 2
  link1:
    url: https://arxiv.org/abs/2210.04429
    display: Paper
  link2:
    url: https://youtu.be/TWWv7fh0Slk
    display: Video

- title: Appearance Consistent Human Pose Transfer via Dynamic Feature Selection
  image: Pose_transfer.png
  description: We present a robut deep architecture for Appearance Consistent person image generation in novel poses. We incorporate a 3 stream network, for image, pose, and appearance. Additionaly we use Gated convolutions and, Non-local attention blocks for generating realistic images.
  authors: Ashish Tiwari, <b>Zeeshan Khan</b>, <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a>
  venue: <i>In International Conference on Pattern Recognition (<b>ICPR</b>), 2022</i>
  number_link: 1
  link1:
    url: https://zeeshank95.github.io/
    display: Paper

- title: Exploring Pair-Wise NMT for Indian Languages
  image: ICON.png
  description: We address the task of improving pair-wise machine translation for low resource Indian languages using a filtered back-translation process and subsequent fine-tuning on the limited pair-wise language corpora
  authors: Kartheek Akella, Sai Himal Allu, Sridhar Suresh Ragupathi, Aman Singhal,<b>Zeeshan Khan</b>, <a href="https://vinaypn.github.io"> Vinay Namboodiri</a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a>
  venue: <i>In International Conference on Natural Language Processing(<b>ICON</b>) 2020</i>
  number_link: 1
  link1:
    url: https://aclanthology.org/2020.icon-main.59/
    display: Paper

- title: "FHDR: HDR Image Reconstruction from a Single LDR Image using Feedback Network"
  image: HDR_img.png
  description: Proposed a recurrent Feedback CNN for HDR image reconstruction from a single exposure LDR image, achieving SOTA results on all the HDR benchmarks. Designed a novel Dense Feedback Block using hidden states of RNN, to transfer the high-level information to the low-level features. LDR to HDR representations are learned in multiple iterations via feedback loops.
  authors: <b>Zeeshan Khan</b>, Mukul khanna, and <a href="https://people.iitgn.ac.in/~shanmuga/">Prof. Shanmuganathan Raman</a>
  venue: <i>In Global Conference on Signal and Information Processing (<b>GlobalSIP</b>) 2019 <span style="color:red;">(ORAL)</span></i> 
  number_link: 2
  link1:
    url: https://arxiv.org/pdf/1912.11463.pdf
    display: Paper    
  link2:
    url: https://github.com/mukulkhanna/FHDR
    display: Code (Github)    